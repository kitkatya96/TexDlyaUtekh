\documentclass[12pt,a4paper]{scrartcl}
\usepackage{my}
\renewcommand{\theequation}{
	\arabic{equation}
}
\usepackage{amstext} % for \text macro
\usepackage{array}   % for \newcolumntype macro
\usepackage{pb-diagram}
\usepackage{tikz}
\usepackage{tabu}
\newcolumntype{L}{>{$}c<{$}} % math-mode version of "l" column type

\begin{document}
	
	\tableofcontents
	\pagebreak
	\section{Введение.}
	Фундаментальные величины теории информации  - энтропия, относительная энтропия и взаимная информация — в теории вероятностей выражаются функциями распределения случайных величин. Они характеризуют поведение длинных последовательностей случайных величин и позволяют нам оценить вероятности наступления редких событий (large deviation theory) и определять показатель ошибки при тестировании гипотез с наименьшим отклонением.\\
	В разработке механизмов компьютерной диагностики (CAD) важным моментов является поиск и отбор релевантных признаков согласно заданым критериям. Чем детальней продуман алгоритм решения такой задачи поиска, тем меньше вероятность получить расхождение в итоговых данных и результатах диагностики.
	Существует несколько подходов к решению задачи отбора признаков. Кроме алгоритмов, основанных на уменьшении размерности (количества переменных) (LDA) датасетов, сокрашении выборки (предполагается, что величины, имеющие минимальный вес не влияют на ответ) - weight pruning, а также GA ( generic algorithms ) - некоторой формы искусственного интелекта, создающей популяции случайных величин и их свойств (каждая популяция предполагается как итоговое решение), имеет место метод, основанный на взаимной информации, как критерия отбора признаков. Данный метод имеет некоторые преимущества, в том числе потомy-что не зависит от алгоритма принятия решения, тем самым уменьшая вычислительную сложность в противовес GA.\\
	В данной работе будут разобраны основные понятия теории информации, также затрагиваемые в курсе теории вероятности и статистики, для лучшего понимания материала в дальнейшем, при работе с методами визуализации информации, распознавания образов, нахождения общих признаков при наличии больших объемов данных.
	\pagebreak
	\section{Энтропия и ee свойства. Энтропия и информация}
	\subsection{Основные определения.}
	Энтропия дискретной случайной величины определяется следующим образом:
	\begin{equation}
	H_b(X) = -\sum_{x \in X}^{} p(x) \log_{b}{p(x)},
	\end{equation}
	где от основания логарифа $b$ зависит мера ее измерения.
	В случае, если $b = 2$, количество информации будет измеряться в битах, при $b = 3$ - в тритах, и т. д. Для применения в задачах математической статистики удобнее принимать $b = e$, тогда единицей измерения энтропии будет величина, равная количеству информации в системе по отношениию к элементарной системе - {\itshape nat\/}, содержащей одно состояние.
	
	К примеру, энтропия события с подбрасыванием монеты равна 1 биту или примерно 0.693 нат:
	\[
	-\frac{1}{2}\log_2{\frac{1}{2}} + \left(-\frac{1}{2}\log_2{\frac{1}{2}}\right) = 2 \frac{1}{2}\log_2{2} = 1 \text{ bit}
	\]
	\[
	-\frac{1}{2}\ln{\frac{1}{2}} + \left(-\frac{1}{2}\ln{\frac{1}{2}}\right) \approx 2 \frac{1}{2} 0.693 \approx 0.693 \text{ nat}
	\]
	Стоит принять к сведению, что $H(X)$ является функционалом для распределения случайной величины X, и не зависит от принимаемых ею значений - только от вероятностей.
	Если математическое ожидание можно вычислить по формуле:
	\[
	E_p g(X) = \sum_{x \in X} g(x)p(x),\text{ при } p(x) \sim X,\\
	\]
	\[
	\text{ где } E_p g(x) \text{ - мат. ожидание сл. величины } g(x),
	\]
	то можно интерпретировать результат иначе и представить энтропию $H(X)$ как мат. ожидание случайной величины $\ln{\frac{1}{p(X)}}$:
 	\[
	H(X) = E_p\ln{\frac{1}{p(X)}}.
	\]
	Определим главные свойства для $H(X)$:\\
	1. 
	\begin{equation} \label{eq1}
	\begin{aligned}
	& 1.1 & & 0 \le H(X) \le \log{|X|} \\
	& 1.2 & & H_b(X) = \log_b{a} H_a(X) \\
	\end{aligned}
	\end{equation}
	Пример 1.1:\\
	Пусть
	\[
	H(X) =
	\begin{cases}
	1,& \text{с вер-ю {\itshape p}},\\
	0,& \text{при {\itshape 1-p}},
	\end{cases}
	\]
	при $X \sim p(x)$, тогда
	\[
	H(X) = -p\log{p} - (1-p)\log{1-p} \stackrel{опр.}{=} H(p)
	\]
	Логично было бы сделать вывод, что при $p=1/2$ H (неопределенность) принимает максимальное значение.
	Значение $H(X)$ варьируется в пределах $[0, \log{|\mathbb{X}|}]$, при всех $p(x)$ равных 0 и при равных $p(x)$ соответственно. Неотрицательность $H(X)$ исходит из $p(x) \ge 0$.
	Второе доказывается путем применения свойства логарифма.\\
	2. При равных вероятностях событий энтропия возрастает.\\
	3. Ассоциативное свойство. Если существует два варианта исхода события - энтропия суммы вероятностей наступления каждого из вариантов будет равна сумме энтропий:\\
	Пример 1.2\\
	Предположим, что события возникают со следующими вероятностями:
	\[
	\begin{aligned}
		&	p_1 = \frac{1}{2}, & & p_2 = \frac{1}{3}, & & p_3 = \frac{1}{6}; &
	\end{aligned}
	\]
	таким образом, наступление второго, либо третьего события определяется с вероятностями $2/3$, либо $1/3$ соответственно, то есть:
	\[
		H(p_1, p_2, p_3) = H(1/2, 1/2) + \frac{1}{2}H(1/3, 2/3),
	\]
	
	\subsection{Совместная и условная энтропия.}
	Энтропия совместного наступления случайных событий (величин):\\
	Опр.:
	\begin{equation}
	H(X, Y) = - \sum_{x \in X}^{}\sum_{y \in Y}^{}p(x, y)\log{p(x, y)} = - E_ p \log {p(x ,y)}
	\end{equation}
	Опр.:\\
	\begin{equation}
	\begin{split}
	H(Y|X)  & = \sum_{x \in X}^{} p(x) H(Y, X=x)\\
			& = \sum_{x \in X}^{} p(x) (- \sum_{y \in Y}^{} p(y|x) \log{p(y|x)} )\\
			& = -\sum_{x \in X}^{} \sum_{y \in Y}^{} p(y|x) \log p(y|x),\\
			& = -E_p \log{p(y|x)}
	\end{split}
	\end{equation}
	где $p(y|x)$ - функция распределения вероятности наступления события $y$ относительно $x$.
	Условная энтропия показывает ожидаемую степень неопределенностей функций совместного распределения случайных величин относительно друг друга.\\
	Естественно предположить, что степень неопределенности для совместного распределения двух величин можно определить как неопределенность первой, расширенной на количество информации, которая известна ей о второй случайной величине. Доказать это можно следующим образом:
	\[
	\begin{split}
	H(X, Y) & = -\sum_{x \in X}^{} \sum_{y \in Y}^{} p(x, y) \log{p(x, y)} \\
			& \text{подставляя } p(x/y) = \frac{p(x, y)p(y)}{p(x)} \text{( формула условной вероятности события)} \\
			& = -\sum_{x \in X}^{} \sum_{y \in Y}^{} p(x, y) \log{p(y) p(x/y)} \\
			& = -\sum_{x \in X}^{} \sum_{y \in Y}^{} p(x, y) ( \log{p(y)} + \log { p(x/y) } )\\
			& \text{подставляя } \sum_{x \in X}^{} p(x, y) = p(y) \\
			& = - \sum_{y \in Y}^{} p(y) \log{p(y)} - \sum_{x \in X}^{} \sum_{y \in Y}^{} p(x, y) \log { p(x/y) } \\
			& \text{подставляя } H(X|Y) =  \sum_{x \in X}^{} p(x)(- \sum_{y \in Y}^{} p(y|x) \log{p(y|x)} ),  \text{	и }  p(x, y) = p(x/y) p(y)\\
			& = H(Y) + H(X|Y)
	\end{split}
	\]
	Следствие:
	\[
	H(X, Y|Z) = H(X|Z) + H(Y|X,Z)
	\]
	Пример 1.3:\\
	%\begin{tabular}{| L | L | L | L |}
	%	\hline
	%	\frac{1}{8} & \frac{3}{8} & \frac{3}{8} & \frac{1}{8} \\
	%\end{tabular}

	\[
	\begin{tabu}{c | c c c c}
	& 1 & 2 & 3 & 4 \\
	\hline
	1 & \frac{1}{4} & \frac{1}{8} & \frac{1}{16} & \frac{1}{16} \\
	2 & \frac{1}{8} & \frac{1}{4} & \frac{1}{16} & \frac{1}{16} \\
	3 & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{8} \\
	4 & \frac{1}{2} & 0 & 0 & 0 \\
	\hline
	\end{tabu}
	\]
	\hangindent=2cm \hangafter=-2 \noindent По формуле общей вероятности,
	\[
	P(X) = \Big( \frac{8}{8}, \frac{4}{8}, \frac{4}{16}, \frac{4}{16} \Big), \text{ и }
	P(Y) = \Big( \frac{8}{16}, \frac{8}{16}, \frac{4}{8}, \frac{1}{2} \Big).
	\]
	\[
	\begin{aligned}
		& H(X) = \sum_{i = 1}^{4} p(x = i) \log_{2}p(x = i) = \frac{3}{2}\\
		& H(Y) = \sum_{i = 1}^{4} p(y = i) \log_{2}p(y = i) = 2.
	\end{aligned}
	\]
	\[
	\begin{aligned}
	H(X|Y) = \sum_{i = 1}^{4} p(Y = i) H (X|Y = i) = \frac{1}{2}H(\frac{8}{8}, \frac{4}{8}, \frac{4}{16}, \frac{4}{16}) + \frac{1}{2}H()
	\end{aligned}
	\]
	
	\subsection{Относительная энтропия и взаимная информация.}
	
	Относительная энтропия является мерой расстония между двумя распределениями и определяет, насколько неэффективно предположение о том, что распределение равно $q$, когда на самом деле оно равно $p$. К примеру, если нам известно вероятностное распределение события $x$, прирост информации будет равен $H(x)$. Если вместо него мы используем функцию для распределения $y$, прирост или число неопределенности увеличиться на $D(x || y)$.\\
	
	Опр.: \ {\itshape Относительная энтропия\/} (Дивергенция) или {\itshape расстояние Кульбака-Лейблера\/} между двумя функциями распределения по-другому может трактоваться как информационное расхождение и имеет вид::
	\begin{equation}
	\begin{split}
	D(p || q ) & = \sum_{x \in }^{} p(x) \ln {\frac{p(x)}{q(x)}}\\
	& = E_p \ln {\frac{p(X)}{q(X)}},
	\end{split}
	\end{equation}
	,где $p(x)$ и $q(x)$ - функции вероятностного распределения. 
	
	Опр:  \ {\itshape Взаимная информация\/} есть относительная энтропия между совместным распределением двух случайных величин и произведением их распределений:
	
	\begin{equation}
	\begin{split}
		I(X, Y) & = \sum_{x \in X}^{} \sum_{y \in Y}^{} p(x, y) \ln{\frac{p(x, y)}{p(x)p(y)}} \\
				& = D(p(x, y) || p(x, y)). \\
				& = E_{p(x, y)} \log{\frac{p(x, y)}{p(x)p(y)}}.
	\end{split}
	\end{equation}
	Пример 1.2.1: \\
	\[
	\begin{aligned}
	X = \{ 0, 1 \} & & p(x) = 
	\end{aligned}
	\left\{
	\begin{aligned}
	& 1 - r, & x = 0\\
	& r, & x = 1.
	\end{aligned}
	\right.
	q(x) = \left\{
	\begin{aligned}
	& 1 - s, & x = 0\\
	& s, & x = 1.
	\end{aligned}
	\right.\\
	\]
	\[
	\begin{split}
	D(p || q ) = & p(0)\log_{2}\frac{p(0)}{q(0)} + p(1)\log_{2}\frac{p(1)}{q(1)} = \\
				= & (1-r)\log_{2}\frac{1-r}{1-s} + r\log_{2}\frac{r}{s}.
	\end{split}
	\]

	\section{Энтропия непрерывных случайных величин. Совместная энтропия и информация, взаимная энтропия.}
	Понятия и свойства, относящиеся к энтропии дискретных и непрерывных величин довольно схожи, однако имеют некоторые различия.\\
	Пусть задана некоторая случайная величина $\psi$, принимающая значения на интервале вещественной оси, причем  для нее определена функция, $p_{\psi}(x)$, такая, что:
	\[
	P \{ a < x \le b \} = \int_{a}^{b} p_{\psi}(x) dx,
	\]
	называемая плотностью распределения $F_{\psi}(x)$.
	Опр.: Энтропия непрерывной случайной величины $\psi$ имеет вид:\\
	\begin{equation}
	\begin{aligned}
		H_{\psi}(x) = \int_{\Omega}^{} p(x) \log_{b} p(x) dx, & & \Omega = \left\{ x : x \in [a, b] \right\}
	\end{aligned}
	\end{equation}
	В данном случае $H$ зависит от вероятностного распределения величины $ \psi $, то есть от плотности $ P(x) $.\\
	Пример 2.1.1:\\
	Для равномерного распределения:\\
	\[
	F(X) = \left\{
	\begin{aligned}
		& \frac{1}{a}, x \in [0, a], &&\\
		& 0, x \notin [0, a]. &&
	\end{aligned}
	\right.
	h(x) = \int_{0}^{a} \frac{1}{a} \ln{\frac{1}{a}} dx = \ln \frac{1}{a}.
	\]
	Примечание:\\
	Хотя мы и определили энтропию дискретной случайной величины как неотрицательную функию, в данном случае при a < 0 значение $h(x)$ будет отрицательным, однако $e^{\ln a} = a$ - это объем дополнения (support set), значение всега положительно.\\
	\linebreak
	Пример 2.1.2:\\
	Для нормального распределения общего вида:\\
	\[
	\begin{aligned}
	 x \sim p_0(x), & & p_0(x) = \frac{1}{\sqrt{ 2 \pi \sigma^2 } }\exp - \frac{(x - a)^2}{2 \sigma^2}, \\
	\end{aligned}
	\]
	\[ 
	\begin{split}
	h(p_0) = - \int p_0 \ln p_0 dx = & - \int p_0(x) \left[ - \frac{(x - a)^2}{2 \sigma^2} - \ln \sqrt{ 2 \pi \sigma^2 } \right] dx = \\
	& = \ln \sqrt{2\pi\sigma^2} \int p_0(x)dx + \frac{1}{2 \sigma^2} \int (x - a)^2 p_0(x)dx =
	\end{split}
	\]
	\[
	\left| \text{ учитывая: } \sigma^2 = \int (x - a)^2 p_0(x), \text{ и применяя свойство нормировки, получим: } \right| \\
	\]
	\[
	= \frac{1}{2} \ln 2 \pi \sigma^2 + \frac{1}{2} = \\
	\]
	\[
	\begin{aligned}
	= & \frac{1}{2} \ln 2 \pi \sigma^2 + \frac{1}{2} \ln e = \frac{1}{2} \ln 2 \pi e \sigma^2\\.
	\end{aligned}
	\]
	\subsection{Связь энтропии непрерывных и дискретных случайных величин.}
	Пусть задана плотность $p(x)$ случайной величины $\psi$ на промежутке [a, b]. Разделим данный промежуток на подотрезки равной длины $\Delta$, и предположим, что функция $p$ также непрерывна на каждом из $\Delta$. Тогда существует такое $x_i \in \Delta$, что:
	\[
	p(x_i) \Delta = \int_{i*\Delta}^{(i+1)*\Delta} p(x) dx.
	\]
	% где то здесь запихни график
	Теперь введем переменную $X$:
	\[
	X^{\Delta} = x_i \iff i*\Delta \le X < (i+1)*\Delta 
	\]
	, получим вероятность на каждом промежутке:
	\[
	p_i = \int_{i*\Delta}^{(i+1)*\Delta} p(x) dx = p(x_i)\Delta
	\]
	, проведя процедуру квантования. Посчитаем энтропию для данного разбиения:
	\[
	\begin{split}
	H(X^{\Delta}) 	& = - \sum p_i \ln p_i = - \sum p(x_i)\Delta \ln p(x_i)\Delta = \\
				 	& = - \sum p(x_i)\Delta \left( \ln p(x_i) + \ln \Delta \right) = \\
				 	& = - \sum \Delta p(x_i) \ln p(x_i) - \ln \Delta \sum p(x_i) \Delta = \\
				 	& = - \sum \Delta p(x_i) \ln p(x_i) - \ln \Delta. (*_1),
	\end{split}
	\] 
	т. к. $\sum p(x_i) \Delta = \int p(x_i) dx =  1$.
	Если
	\[
	p(x) \log p(x) \in \Re \text{ - интегрируемо по Риману,} 
	\]
	то выражение $(*_1)$ стремится к значению 
	\[
    -p(x_i) \ln p(x_i), \text{ т. к. } \Delta \rightarrow 0 \text{ согласло свойству интеграла Римана.}
	\]
	Отразим данный факт в следующей теореме:\\
	Th.: Если функция $f(x)$ распределения случайной величины $X$ интегрируема по Римнау, тогда:
	\[
	H(X^{\Delta}) + \log \Delta \rightarrow h(f) = h(X), \text{ при } \Delta \rightarrow 0. 
	\]
	Таким образом, энтропия после проведения квантования возрастает примерно на $\log \Delta \approx n$.\\
	Пример 2.2.1:\\
	Задано равномерное распределение на [0, 1]:\\
	\[
	\xi \sim f(x) =
	\left\{
	\begin{aligned}
	& 1, x \in [0,1],\\
	& 0, x \notin [0,1].
	\end{aligned}
	\right.
	\text{ и } \Delta = 2^{-n}.
	\]
	\[
	\begin{aligned}
	& h(x) = - \int_{0}^{1} f(x) \log_{2}f(x) = - \int_{0}^{1} 1 \log_{2} 1 = 0. \\
	& H(X^{\Delta}) = - \sum \Delta f(x) \ln f(x) - \ln \Delta = 0 - \log_{2} 2^{-n} = n \text{ бит.}
	\end{aligned}
	\]
	Пример 2.2.2:\\
	\[
	\Delta = e^{-n} \text{ , } \xi \sim f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp {-\frac{(x - a)^2}{\sigma^2}} \text{ - нормальное р-е}.
	\]
	\[
	\begin{aligned}
	& h(X) = \frac{1}{2} \ln 2 \pi e \sigma^2.\\
	& H(X^{\Delta}) = h(X) + n = \frac{1}{2} \ln 2 \pi e \sigma^2 + n \text{ нат.}
	\end{aligned} 
	\]
	Th.(Экстремальное свойство)\\
	Если заданы два распределения:
	\[
	\begin{aligned}
	p_0(x) \sim \space N(a, \sigma^2) \text{ - Гаусовское распределение, } &  q(x) \text{ - любое другое, то } \\
	\end{aligned}
	\]
	\[
	H(q) \le H(p_0).
	\]
	доказательство:\\
	\[
	\begin{aligned}
	& H = H(q), & H_0 = H(p_0) & \rightarrow H - H_0 = \int p_0 \ln p_0 + \int q \ln q + ( \int q \ln p_0 - \int q \ln p_0) = \\
	\end{aligned}
	\]
	\[
	\begin{aligned}
	& = \int (p_0 - q) \ln p_0 - \int q (\ln q - \ln p_0) = \\
	& = \int (p_0 - q) \ln \left\{ \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left(-\frac{(x - a)^2}{2\sigma^2} \right) \right\} - \int q \ln \frac{q}{p_0}.\\
	& = \int (p_0 - q) \left[ const -\frac{(x - a)^2}{2\sigma^2} \right]dx + \int q \ln \left[ \frac{p_0}{q} - 1 \right]dx = *\\
	\end{aligned}
	\]
	Учитывая:
	\[
	\begin{aligned}
	a = \int_{-\infty}^{\infty} x q(x) dx, & & \sigma^2 = \int_{-\infty}^{\infty} (x - a)^2 q(x) dx,\\
	\text{также, что } \ln x \le x - 1,
	\end{aligned}
	\]
	получаем:\\
	\[
	\begin{aligned}
	& * \le \int (p_0 - q) \left[ const -\frac{(x - a)^2}{2\sigma^2} \right]dx + \int q \left( \frac{p_0}{q} - 1 \right) dx \le 0.
	\end{aligned}
	\]
	\subsection{Совместная и условная энтропия непрерывных случайных величин.}
	Опр.: \\
	\begin{equation}
	h(X_1, X_2,..., X_n) = -\int f(x^n) \log f(x^n) dx^n. \text{ - cовместная энтропия.}
	\end{equation}
	\[
	X_1,...X_n \sim f(x_1,...x_n)
	\]
	Опр.:\\
	\begin{equation}
	h(X|Y) = - \int f(x, y) \log f(x| y) dx dy \text{ условная энтропия.}
	\end{equation}
	\[
	X, Y \sim f (x, y) \text{ - совместное распределение.}
	\]
	Если для двух случайных величин $\psi$ и $\eta$ задано распределение вида:\\
	\[
	p_{\psi,\eta}(x, y) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-r^2}}\exp \left\{-\frac{1}{2(1-r^2)} \left(\frac{x - a_1}{\sigma_1}\right)^2 - 2r\frac{x-a_1}{\sigma_1}\frac{y - a_2}{\sigma_2} + \left(\frac{y - a_2}{\sigma_2}\right)^2 \right\},
	\]
	- 2-мерное нормальное распределение. Ведем обозначение:
	\[
	Q = \left(\frac{x - a_1}{\sigma_1}\right)^2 - 2r\frac{x-a_1}{\sigma_1}\frac{y - a_2}{\sigma_2} + \left(\frac{y - a_2}{\sigma_2}\right)^2
	\]
	\[
	\begin{aligned}
	H(x, y) = & - \int\int p(x, y) \ln p(x, y) dx dy = \\
	= & - \int\int p(x, y) \left\{ -\ln \left( 2 \pi \sigma_1 \sigma_2 \sqrt{1-r^2} \right) - \frac{1}{2(1-r^2)}Q \right\} dx dy\\
	= & \ln \left( 2 \pi \sigma_1 \sigma_2 \sqrt{1-r^2} \right) \int\int p(x, y) dx dy + \frac{1}{2(1-r^2)} \int\int p(x, y) [Q] dx dy =
	\end{aligned}
	\]
	и подставляя значения $\sigma_1^2$, $\sigma_2^2$, $a_1$, $a_2$, $r$, а также пользуясь свойством нормировки $p(x, y)$, получаем:\\
	\[
	\begin{aligned}
	& a_1 = \int x p(x) dx & & a_2 = \int y p(y) dy \\
	\end{aligned}
	\]
	\[
	\begin{aligned}
	& \sigma_1 = \int p(x) (x - a_1)^2 dx & & \sigma_2 = \int p(y) (y -a_2)^2 dy
	\end{aligned}
	\]
	\[
	 r = \frac{1}{\sigma_1\sigma_2} \int \int (x - a_1)(y - a_2) dxdy
	\]
	\[
	\begin{aligned}
	&*_1 \frac{1}{2(1-r^2)}  \int\int p(x, y) [Q] dx dy = \\
	& = \frac{1}{2(1-r^2)} \int\int p(x, y) [Q] \left[ \left(\frac{x - a_1}{\sigma_1}\right)^2 - 2r\frac{x-a_1}{\sigma_1}\frac{y - a_2}{\sigma_2} + \left(\frac{y - a_2}{\sigma_2}\right)^2 \right] =\\
	& = \frac{1}{2(1-r^2)} \left[ 2 - 2r^2 \right] = 1
	\end{aligned}
	\]
	\[
	\begin{aligned}
	H(x, y) & = \ln \left( 2 \pi \sigma_1 \sigma_2 \sqrt{1-r^2} \right) + 1 \\
	& = \ln \sqrt{2\pi\sigma_1^2} + \ln \sqrt{2\pi\sigma_2^2} + \ln\sqrt{1 - r^2} + 1.
	\end{aligned}
	\]
	\subsection{Относительная энтропия и взаимная информация непрерывных распределений.}
	Опр.: расст. Кульбака-Лейблера
	\begin{equation}
	D(f||g) = \int f(x) \log \frac{f(x)}{g(x)}
	\end{equation}
	прим.: относительная энтропия имеет предел только в случае если множество - носитель функции* $f$ - $supp\left\{f\right\}$ содержится в $supp\left\{g\right\}$. \\
	*Носитель функции -
	\[
	supp\left\{y\right\} = \left\{x:y(x) \not = 0 \right\}
	\] 
	- множество, на котором функция не обращается в ноль.\\
	Опр.: \\
	\begin{equation}
		I(X, Y) = \int \int f(x, y) \log \frac{f(x, y)}{f(x) f(y)} dx dy
	\end{equation}
	или
	\[
		I(X, Y) = D (f (x, y) || f(x) f(y).
	\]
	- взаимная информация двух случайных величин, имеющих совместное распределение $f$.
	Учитывыя, что
	\[
	\int \int f(x, y) dx dy = \int f(x) dx \text{ } [ = \int f(y) dy]
	\]
	и раскрывая логарифм в первом выражении, получим:
	\[
	\begin{aligned}
		I(X, Y) & = \int f(x, y) \log f(x, y) - \int f(x, y) \log f(x) - \int f(x, y) \log f(y) = \\
		& = - h(X, Y) + h(X) + h(Y) = h(X) - h(X|Y) =\\
		& \left[ = h(Y) - h(Y | X) \right].
	\end{aligned}
	\]
	Все свойства определенные для (1) и (2) дискретных величин равносильны и в случае непрерывных распределений. В частности, пределом $I(X^{\Delta}, Y^{\Delta})$ (в случае квантования)  будет инф-я непрерывных $x$ и $y$:
	\[
	\begin{aligned}
		I(X^{\Delta}, Y^{\Delta}) & = H(X^{\Delta}) - H(X^{\Delta}|Y^{\Delta}) = \\
		& \approx -h(X|Y) + h(X) - \log \Delta + \log \Delta= \\
		& \approx  h(X) - \log \Delta - \left( -h(X|Y)) - \log \Delta \right) \\
		& = I(X, Y).
	\end{aligned}
	\]
	\begin{thebibliography}{3}
		\bibitem{Cover}
		Cover, T. M., // Elements of information theory/by Thomas M. Cover, Joy A. Thomas.–2nd ed. --- 1938 
		\bibitem{Visualization}
		Chen, Min, // Information theory tools for visualization --- Min Chen, 1960
	\end{thebibliography}
\end{document}